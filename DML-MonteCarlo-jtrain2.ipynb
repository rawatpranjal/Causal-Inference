{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4a5c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.87673972\n",
      "Iteration 2, loss = 2.81772548\n",
      "Iteration 3, loss = 0.97819756\n",
      "Iteration 4, loss = 1.46328709\n",
      "Iteration 5, loss = 1.00752766\n",
      "Iteration 6, loss = 0.68627108\n",
      "Iteration 7, loss = 0.86141401\n",
      "Iteration 8, loss = 0.71117988\n",
      "Iteration 9, loss = 0.70135705\n",
      "Iteration 10, loss = 0.76582677\n",
      "Iteration 11, loss = 0.73543689\n",
      "Iteration 12, loss = 0.68300651\n",
      "Iteration 13, loss = 0.69268679\n",
      "Iteration 14, loss = 0.72514142\n",
      "Iteration 15, loss = 0.71792968\n",
      "Iteration 16, loss = 0.68882130\n",
      "Iteration 17, loss = 0.67749800\n",
      "Iteration 18, loss = 0.68885446\n",
      "Iteration 19, loss = 0.69874122\n",
      "Iteration 20, loss = 0.69413188\n",
      "Iteration 21, loss = 0.68171368\n",
      "Iteration 22, loss = 0.67610814\n",
      "Iteration 23, loss = 0.67774236\n",
      "Iteration 24, loss = 0.68188685\n",
      "Iteration 25, loss = 0.68231328\n",
      "Iteration 26, loss = 0.67852480\n",
      "Iteration 27, loss = 0.67455167\n",
      "Iteration 28, loss = 0.67331370\n",
      "Iteration 29, loss = 0.67477121\n",
      "Iteration 30, loss = 0.67649672\n",
      "Iteration 31, loss = 0.67641246\n",
      "Iteration 32, loss = 0.67439505\n",
      "Iteration 33, loss = 0.67239288\n",
      "Iteration 34, loss = 0.67191352\n",
      "Iteration 35, loss = 0.67272172\n",
      "Iteration 36, loss = 0.67309212\n",
      "Iteration 37, loss = 0.67217598\n",
      "Iteration 38, loss = 0.67067184\n",
      "Iteration 39, loss = 0.66962208\n",
      "Iteration 40, loss = 0.66953415\n",
      "Iteration 41, loss = 0.66988419\n",
      "Iteration 42, loss = 0.66985408\n",
      "Iteration 43, loss = 0.66882162\n",
      "Iteration 44, loss = 0.66780545\n",
      "Iteration 45, loss = 0.66742098\n",
      "Iteration 46, loss = 0.66736530\n",
      "Iteration 47, loss = 0.66728381\n",
      "Iteration 48, loss = 0.66676257\n",
      "Iteration 49, loss = 0.66590226\n",
      "Iteration 50, loss = 0.66536658\n",
      "Iteration 51, loss = 0.66509936\n",
      "Iteration 52, loss = 0.66513311\n",
      "Iteration 53, loss = 0.66452719\n",
      "Iteration 54, loss = 0.66395901\n",
      "Iteration 55, loss = 0.66347371\n",
      "Iteration 56, loss = 0.66308888\n",
      "Iteration 57, loss = 0.66293896\n",
      "Iteration 58, loss = 0.66276955\n",
      "Iteration 59, loss = 0.66225359\n",
      "Iteration 60, loss = 0.66200486\n",
      "Iteration 61, loss = 0.66155004\n",
      "Iteration 62, loss = 0.66118529\n",
      "Iteration 63, loss = 0.66092679\n",
      "Iteration 64, loss = 0.66052159\n",
      "Iteration 65, loss = 0.66047675\n",
      "Iteration 66, loss = 0.65984761\n",
      "Iteration 67, loss = 0.65950226\n",
      "Iteration 68, loss = 0.65911705\n",
      "Iteration 69, loss = 0.65877598\n",
      "Iteration 70, loss = 0.65848144\n",
      "Iteration 71, loss = 0.65793554\n",
      "Iteration 72, loss = 0.65756366\n",
      "Iteration 73, loss = 0.65719080\n",
      "Iteration 74, loss = 0.65586649\n",
      "Iteration 75, loss = 0.65507180\n",
      "Iteration 76, loss = 0.65535082\n",
      "Iteration 77, loss = 0.65482859\n",
      "Iteration 78, loss = 0.65342274\n",
      "Iteration 79, loss = 0.65323462\n",
      "Iteration 80, loss = 0.65324785\n",
      "Iteration 81, loss = 0.65238750\n",
      "Iteration 82, loss = 0.65010670\n",
      "Iteration 83, loss = 0.65034488\n",
      "Iteration 84, loss = 0.65188546\n",
      "Iteration 85, loss = 0.65127713\n",
      "Iteration 86, loss = 0.64735623\n",
      "Iteration 87, loss = 0.64939449\n",
      "Iteration 88, loss = 0.64835498\n",
      "Iteration 89, loss = 0.64570505\n",
      "Iteration 90, loss = 0.64866316\n",
      "Iteration 91, loss = 0.64563440\n",
      "Iteration 92, loss = 0.64373642\n",
      "Iteration 93, loss = 0.64613835\n",
      "Iteration 94, loss = 0.64216486\n",
      "Iteration 95, loss = 0.64201411\n",
      "Iteration 96, loss = 0.64294321\n",
      "Iteration 97, loss = 0.63904313\n",
      "Iteration 98, loss = 0.63875307\n",
      "Iteration 99, loss = 0.63972724\n",
      "Iteration 100, loss = 0.63722875\n",
      "Iteration 101, loss = 0.63476134\n",
      "Iteration 102, loss = 0.63510974\n",
      "Iteration 103, loss = 0.63632250\n",
      "Iteration 104, loss = 0.63394606\n",
      "Iteration 105, loss = 0.63093511\n",
      "Iteration 106, loss = 0.62950320\n",
      "Iteration 107, loss = 0.62968981\n",
      "Iteration 108, loss = 0.63187819\n",
      "Iteration 109, loss = 0.63667877\n",
      "Iteration 110, loss = 0.64190996\n",
      "Iteration 111, loss = 0.62962687\n",
      "Iteration 112, loss = 0.62235922\n",
      "Iteration 113, loss = 0.62669428\n",
      "Iteration 114, loss = 0.63145465\n",
      "Iteration 115, loss = 0.63736252\n",
      "Iteration 116, loss = 0.62147053\n",
      "Iteration 117, loss = 0.62198219\n",
      "Iteration 118, loss = 0.63037159\n",
      "Iteration 119, loss = 0.61839748\n",
      "Iteration 120, loss = 0.61899552\n",
      "Iteration 121, loss = 0.62436736\n",
      "Iteration 122, loss = 0.61593419\n",
      "Iteration 123, loss = 0.61337702\n",
      "Iteration 124, loss = 0.61979758\n",
      "Iteration 125, loss = 0.61555327\n",
      "Iteration 126, loss = 0.60813624\n",
      "Iteration 127, loss = 0.60812908\n",
      "Iteration 128, loss = 0.61060928\n",
      "Iteration 129, loss = 0.61120478\n",
      "Iteration 130, loss = 0.61136195\n",
      "Iteration 131, loss = 0.61218376\n",
      "Iteration 132, loss = 0.60974321\n",
      "Iteration 133, loss = 0.60797619\n",
      "Iteration 134, loss = 0.60248464\n",
      "Iteration 135, loss = 0.59761408\n",
      "Iteration 136, loss = 0.59834439\n",
      "Iteration 137, loss = 0.60178950\n",
      "Iteration 138, loss = 0.60681619\n",
      "Iteration 139, loss = 0.61059148\n",
      "Iteration 140, loss = 0.62245028\n",
      "Iteration 141, loss = 0.61172079\n",
      "Iteration 142, loss = 0.59426001\n",
      "Iteration 143, loss = 0.59268647\n",
      "Iteration 144, loss = 0.60653746\n",
      "Iteration 145, loss = 0.60486686\n",
      "Iteration 146, loss = 0.59134403\n",
      "Iteration 147, loss = 0.58566653\n",
      "Iteration 148, loss = 0.58920032\n",
      "Iteration 149, loss = 0.58903890\n",
      "Iteration 150, loss = 0.58587516\n",
      "Iteration 151, loss = 0.58057131\n",
      "Iteration 152, loss = 0.57983152\n",
      "Iteration 153, loss = 0.58671632\n",
      "Iteration 154, loss = 0.59233320\n",
      "Iteration 155, loss = 0.60524175\n",
      "Iteration 156, loss = 0.58814442\n",
      "Iteration 157, loss = 0.57388515\n",
      "Iteration 158, loss = 0.58002596\n",
      "Iteration 159, loss = 0.58823188\n",
      "Iteration 160, loss = 0.59636747\n",
      "Iteration 161, loss = 0.58213827\n",
      "Iteration 162, loss = 0.56940110\n",
      "Iteration 163, loss = 0.57931031\n",
      "Iteration 164, loss = 0.59672808\n",
      "Iteration 165, loss = 0.61417548\n",
      "Iteration 166, loss = 0.59051064\n",
      "Iteration 167, loss = 0.57423441\n",
      "Iteration 168, loss = 0.59299477\n",
      "Iteration 169, loss = 0.57622972\n",
      "Iteration 170, loss = 0.57409898\n",
      "Iteration 171, loss = 0.58832689\n",
      "Iteration 172, loss = 0.56883923\n",
      "Iteration 173, loss = 0.56709322\n",
      "Iteration 174, loss = 0.58008244\n",
      "Iteration 175, loss = 0.57216933\n",
      "Iteration 176, loss = 0.56040282\n",
      "Iteration 177, loss = 0.55770396\n",
      "Iteration 178, loss = 0.56445417\n",
      "Iteration 179, loss = 0.55973875\n",
      "Iteration 180, loss = 0.55374471\n",
      "Iteration 181, loss = 0.55209497\n",
      "Iteration 182, loss = 0.55781633\n",
      "Iteration 183, loss = 0.56837175\n",
      "Iteration 184, loss = 0.57630459\n",
      "Iteration 185, loss = 0.57020142\n",
      "Iteration 186, loss = 0.55575029\n",
      "Iteration 187, loss = 0.54839432\n",
      "Iteration 188, loss = 0.55407859\n",
      "Iteration 189, loss = 0.56699145\n",
      "Iteration 190, loss = 0.56980774\n",
      "Iteration 191, loss = 0.56665399\n",
      "Iteration 192, loss = 0.54591797\n",
      "Iteration 193, loss = 0.54712357\n",
      "Iteration 194, loss = 0.55584334\n",
      "Iteration 195, loss = 0.55228150\n",
      "Iteration 196, loss = 0.54094947\n",
      "Iteration 197, loss = 0.54260661\n",
      "Iteration 198, loss = 0.54876652\n",
      "Iteration 199, loss = 0.55246207\n",
      "Iteration 200, loss = 0.55171404\n",
      "Iteration 201, loss = 0.54472243\n",
      "Iteration 202, loss = 0.53811640\n",
      "Iteration 203, loss = 0.53327029\n",
      "Iteration 204, loss = 0.53682959\n",
      "Iteration 205, loss = 0.53891819\n",
      "Iteration 206, loss = 0.54276309\n",
      "Iteration 207, loss = 0.54176377\n",
      "Iteration 208, loss = 0.54746330\n",
      "Iteration 209, loss = 0.54992424\n",
      "Iteration 210, loss = 0.54330334\n",
      "Iteration 211, loss = 0.53928751\n",
      "Iteration 212, loss = 0.52786238\n",
      "Iteration 213, loss = 0.52782774\n",
      "Iteration 214, loss = 0.52677011\n",
      "Iteration 215, loss = 0.53084795\n",
      "Iteration 216, loss = 0.53948785\n",
      "Iteration 217, loss = 0.55098407\n",
      "Iteration 218, loss = 0.55904007\n",
      "Iteration 219, loss = 0.56229552\n",
      "Iteration 220, loss = 0.55604746\n",
      "Iteration 221, loss = 0.52780323\n",
      "Iteration 222, loss = 0.53242741\n",
      "Iteration 223, loss = 0.54617185\n",
      "Iteration 224, loss = 0.54115743\n",
      "Iteration 225, loss = 0.52637116\n",
      "Iteration 226, loss = 0.51959826\n",
      "Iteration 227, loss = 0.53060197\n",
      "Iteration 228, loss = 0.54252906\n",
      "Iteration 229, loss = 0.53104136\n",
      "Iteration 230, loss = 0.52440708\n",
      "Iteration 231, loss = 0.52135541\n",
      "Iteration 232, loss = 0.52713077\n",
      "Iteration 233, loss = 0.52793678\n",
      "Iteration 234, loss = 0.51781168\n",
      "Iteration 235, loss = 0.51495954\n",
      "Iteration 236, loss = 0.54987831\n",
      "Iteration 237, loss = 0.52075952\n",
      "Iteration 238, loss = 0.53095459\n",
      "Iteration 239, loss = 0.57089361\n",
      "Iteration 240, loss = 0.57835921\n",
      "Iteration 241, loss = 0.60838033\n",
      "Iteration 242, loss = 0.65172755\n",
      "Iteration 243, loss = 0.60742010\n",
      "Iteration 244, loss = 0.60383530\n",
      "Iteration 245, loss = 0.58809106\n",
      "Iteration 246, loss = 0.55774676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 46.13310377\n",
      "Iteration 2, loss = 22.69356187\n",
      "Iteration 3, loss = 26.23317081\n",
      "Iteration 4, loss = 22.91341933\n",
      "Iteration 5, loss = 24.30413586\n",
      "Iteration 6, loss = 22.18913798\n",
      "Iteration 7, loss = 22.53971648\n",
      "Iteration 8, loss = 22.13870150\n",
      "Iteration 9, loss = 21.90089986\n",
      "Iteration 10, loss = 22.00215633\n",
      "Iteration 11, loss = 21.76710173\n",
      "Iteration 12, loss = 21.85521355\n",
      "Iteration 13, loss = 21.78052503\n",
      "Iteration 14, loss = 21.65227168\n",
      "Iteration 15, loss = 21.69727541\n",
      "Iteration 16, loss = 21.58115227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 21.51082832\n",
      "Iteration 18, loss = 21.52714540\n",
      "Iteration 19, loss = 21.41552616\n",
      "Iteration 20, loss = 21.39469498\n",
      "Iteration 21, loss = 21.37873718\n",
      "Iteration 22, loss = 21.28546719\n",
      "Iteration 23, loss = 21.27521163\n",
      "Iteration 24, loss = 21.20324764\n",
      "Iteration 25, loss = 21.17660658\n",
      "Iteration 26, loss = 21.14461116\n",
      "Iteration 27, loss = 21.09134314\n",
      "Iteration 28, loss = 21.09030803\n",
      "Iteration 29, loss = 21.04756538\n",
      "Iteration 30, loss = 21.04815424\n",
      "Iteration 31, loss = 20.99662872\n",
      "Iteration 32, loss = 20.98980611\n",
      "Iteration 33, loss = 20.95286921\n",
      "Iteration 34, loss = 20.92852209\n",
      "Iteration 35, loss = 20.88354573\n",
      "Iteration 36, loss = 20.86430783\n",
      "Iteration 37, loss = 20.82304569\n",
      "Iteration 38, loss = 20.79964854\n",
      "Iteration 39, loss = 20.76189164\n",
      "Iteration 40, loss = 20.73153170\n",
      "Iteration 41, loss = 20.68861193\n",
      "Iteration 42, loss = 20.65733135\n",
      "Iteration 43, loss = 20.62155880\n",
      "Iteration 44, loss = 20.56941518\n",
      "Iteration 45, loss = 20.53284167\n",
      "Iteration 46, loss = 20.48791134\n",
      "Iteration 47, loss = 20.44391976\n",
      "Iteration 48, loss = 20.40353292\n",
      "Iteration 49, loss = 20.34791447\n",
      "Iteration 50, loss = 20.28864538\n",
      "Iteration 51, loss = 20.23329462\n",
      "Iteration 52, loss = 20.17445372\n",
      "Iteration 53, loss = 20.12459397\n",
      "Iteration 54, loss = 20.09358671\n",
      "Iteration 55, loss = 20.08071972\n",
      "Iteration 56, loss = 20.07871633\n",
      "Iteration 57, loss = 20.04409860\n",
      "Iteration 58, loss = 19.88394491\n",
      "Iteration 59, loss = 19.82639303\n",
      "Iteration 60, loss = 19.88087869\n",
      "Iteration 61, loss = 19.87575302\n",
      "Iteration 62, loss = 19.81296013\n",
      "Iteration 63, loss = 19.69775994\n",
      "Iteration 64, loss = 19.64683264\n",
      "Iteration 65, loss = 19.70963514\n",
      "Iteration 66, loss = 19.84162126\n",
      "Iteration 67, loss = 19.93734250\n",
      "Iteration 68, loss = 19.64781370\n",
      "Iteration 69, loss = 19.54412340\n",
      "Iteration 70, loss = 19.71864901\n",
      "Iteration 71, loss = 19.71577723\n",
      "Iteration 72, loss = 19.52011812\n",
      "Iteration 73, loss = 19.47705902\n",
      "Iteration 74, loss = 19.58191716\n",
      "Iteration 75, loss = 19.53046163\n",
      "Iteration 76, loss = 19.39857433\n",
      "Iteration 77, loss = 19.42471841\n",
      "Iteration 78, loss = 19.50826511\n",
      "Iteration 79, loss = 19.40149931\n",
      "Iteration 80, loss = 19.30171252\n",
      "Iteration 81, loss = 19.30596475\n",
      "Iteration 82, loss = 19.37586121\n",
      "Iteration 83, loss = 19.52344736\n",
      "Iteration 84, loss = 19.48006324\n",
      "Iteration 85, loss = 19.25936326\n",
      "Iteration 86, loss = 19.19311091\n",
      "Iteration 87, loss = 19.30911121\n",
      "Iteration 88, loss = 19.53029559\n",
      "Iteration 89, loss = 19.27900866\n",
      "Iteration 90, loss = 19.09409759\n",
      "Iteration 91, loss = 19.21916836\n",
      "Iteration 92, loss = 19.28113105\n",
      "Iteration 93, loss = 19.12058953\n",
      "Iteration 94, loss = 19.02195978\n",
      "Iteration 95, loss = 19.11086737\n",
      "Iteration 96, loss = 19.14937215\n",
      "Iteration 97, loss = 18.99877643\n",
      "Iteration 98, loss = 18.94767100\n",
      "Iteration 99, loss = 19.02625623\n",
      "Iteration 100, loss = 19.01780189\n",
      "Iteration 101, loss = 18.92868379\n",
      "Iteration 102, loss = 18.85590781\n",
      "Iteration 103, loss = 18.85796795\n",
      "Iteration 104, loss = 18.89126920\n",
      "Iteration 105, loss = 18.89751994\n",
      "Iteration 106, loss = 18.85003445\n",
      "Iteration 107, loss = 18.77983555\n",
      "Iteration 108, loss = 18.73037883\n",
      "Iteration 109, loss = 18.72394607\n",
      "Iteration 110, loss = 18.74620015\n",
      "Iteration 111, loss = 18.81529746\n",
      "Iteration 112, loss = 18.95669900\n",
      "Iteration 113, loss = 19.29446785\n",
      "Iteration 114, loss = 19.27031836\n",
      "Iteration 115, loss = 19.09267977\n",
      "Iteration 116, loss = 18.61332720\n",
      "Iteration 117, loss = 18.79937904\n",
      "Iteration 118, loss = 19.15876010\n",
      "Iteration 119, loss = 18.69138608\n",
      "Iteration 120, loss = 18.54806220\n",
      "Iteration 121, loss = 18.82034193\n",
      "Iteration 122, loss = 18.62872851\n",
      "Iteration 123, loss = 18.45950969\n",
      "Iteration 124, loss = 18.54129059\n",
      "Iteration 125, loss = 18.62170665\n",
      "Iteration 126, loss = 18.47269124\n",
      "Iteration 127, loss = 18.38120075\n",
      "Iteration 128, loss = 18.46175816\n",
      "Iteration 129, loss = 18.52335348\n",
      "Iteration 130, loss = 18.34718845\n",
      "Iteration 131, loss = 18.26979628\n",
      "Iteration 132, loss = 18.28732184\n",
      "Iteration 133, loss = 18.33189042\n",
      "Iteration 134, loss = 18.31815283\n",
      "Iteration 135, loss = 18.21868448\n",
      "Iteration 136, loss = 18.14918311\n",
      "Iteration 137, loss = 18.07483847\n",
      "Iteration 138, loss = 18.09822135\n",
      "Iteration 139, loss = 18.10453275\n",
      "Iteration 140, loss = 18.13972671\n",
      "Iteration 141, loss = 18.20385183\n",
      "Iteration 142, loss = 18.26576840\n",
      "Iteration 143, loss = 18.56125140\n",
      "Iteration 144, loss = 18.58232288\n",
      "Iteration 145, loss = 18.81600124\n",
      "Iteration 146, loss = 18.01801905\n",
      "Iteration 147, loss = 18.42004145\n",
      "Iteration 148, loss = 19.06361555\n",
      "Iteration 149, loss = 18.03653066\n",
      "Iteration 150, loss = 18.49516027\n",
      "Iteration 151, loss = 18.94001222\n",
      "Iteration 152, loss = 17.98814903\n",
      "Iteration 153, loss = 19.70262953\n",
      "Iteration 154, loss = 19.62939689\n",
      "Iteration 155, loss = 19.16392909\n",
      "Iteration 156, loss = 19.19440681\n",
      "Iteration 157, loss = 18.12836731\n",
      "Iteration 158, loss = 18.65036872\n",
      "Iteration 159, loss = 18.47458008\n",
      "Iteration 160, loss = 18.16323176\n",
      "Iteration 161, loss = 18.40449280\n",
      "Iteration 162, loss = 18.00789933\n",
      "Iteration 163, loss = 18.28039772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from scipy.stats import logistic\n",
    "\n",
    "MC_N = 1\n",
    "MC_θ = np.zeros((MC_N,4))\n",
    "MC_y = np.zeros((MC_N,4))\n",
    "MC_t = np.zeros((MC_N,4))\n",
    "\n",
    "for j in range(MC_N):\n",
    "    import wooldridge\n",
    "    df = wooldridge.data('jtrain2')\n",
    "    #df['avg'] = 0.5 * (df.re74+df.re75)\n",
    "    df = df.dropna()\n",
    "    #df = df[df.avg <= 15]\n",
    "    y = df.re78\n",
    "    t = df.train\n",
    "    x = df[['age', 'educ', 'black', 'hisp', 'married', 're74', 're75']]\n",
    "    \n",
    "    # OLS - Full Estimation\n",
    "    model_OLS = sm.OLS(y, np.c_[t,x])\n",
    "    res = model_OLS.fit()\n",
    "    θ_OLS = res.params[0]\n",
    "    # OLS First Stage: Y\n",
    "    model_OLS = sm.OLS(y, np.c_[x])\n",
    "    res_y = model_OLS.fit()\n",
    "    θ_OLS_y = res_y.params[0]    \n",
    "    # Logistic First Stage\n",
    "    clf = LogisticRegression(random_state=0).fit(x, t)\n",
    "    \n",
    "    # DML Lasso\n",
    "    model_Lasso = LinearDML(discrete_treatment=True, random_state=1, cv=1)\n",
    "    model_Lasso.fit(y, t, X=None,W=x)\n",
    "    θ_DMLL = model_Lasso.intercept_\n",
    "\n",
    "    # DML RF\n",
    "    model_XGB = LinearDML(discrete_treatment=True, cv=1,\n",
    "                          model_y = CatBoostRegressor(learning_rate = 1, max_depth = 8,verbose = False), \n",
    "                          model_t = CatBoostClassifier(learning_rate = 1, max_depth = 8,verbose = False))\n",
    "    model_XGB.fit(y.ravel(), t.ravel(), X=None,W=x)\n",
    "    θ_DMLRF = model_XGB.intercept_\n",
    "    \n",
    "    # DML NN - First Stage\n",
    "    model_NN = LinearDML(discrete_treatment=True, cv =1,\n",
    "                         model_y = MLPRegressor(random_state=1,\n",
    "                                                 hidden_layer_sizes=(500,100,50), \n",
    "                                                 batch_size = x.shape[0],\n",
    "                                                 momentum = 0.95, \n",
    "                                                 max_iter=50000, \n",
    "                                                 learning_rate_init=0.01, \n",
    "                                                 verbose=True), \n",
    "                         model_t = MLPClassifier(random_state=1,\n",
    "                                                 hidden_layer_sizes=(500,100,50), \n",
    "                                                 batch_size = x.shape[0],\n",
    "                                                 momentum = 0.95, \n",
    "                                                 max_iter=50000, \n",
    "                                                 learning_rate_init=0.01, \n",
    "                                                 verbose=True))\n",
    "    model_NN.fit(y.ravel(), t.ravel(), X=None,W=x)\n",
    "    θ_DMLRF = model_NN.intercept_\n",
    "\n",
    "\n",
    "    MC_θ[j] = [θ_OLS, model_Lasso.intercept_, model_XGB.intercept_, model_NN.intercept_]\n",
    "    MC_y[j] = [res_y.rsquared, np.mean(model_Lasso.nuisance_scores_y), np.mean(model_XGB.nuisance_scores_y),np.mean(model_NN.nuisance_scores_y)]\n",
    "    MC_t[j] = [clf.score(x,t), np.mean(model_Lasso.nuisance_scores_t), np.mean(model_XGB.nuisance_scores_t),np.mean(model_NN.nuisance_scores_t)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "31aebd79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------+--------+-----------+--------+\n",
      "|          Var           |  OLS  | DML-L  | DML-Boost | DML-NN |\n",
      "+------------------------+-------+--------+-----------+--------+\n",
      "|         θ_hat          | 1.694 | 1.765  |   1.393   | 1.830  |\n",
      "|    First Stage Y R2    | 0.414 | -0.007 |   0.827   | 0.189  |\n",
      "| First Stage D Accuracy | 0.589 | 0.582  |   0.894   | 0.694  |\n",
      "+------------------------+-------+--------+-----------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Var', 'OLS','DML-L','DML-Boost','DML-NN']\n",
    "a = ['θ_hat']+ np.mean(MC_θ, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "a = ['First Stage Y R2']+ np.mean(MC_y, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "a = ['First Stage D Accuracy']+ np.mean(MC_t, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "table.float_format = '0.3'\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847c801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
