{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4a5c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 4) (428,) (428,)\n",
      "Iteration 1, loss = 132.06658334\n",
      "Iteration 2, loss = 12.55072934\n",
      "Iteration 3, loss = 12.37243333\n",
      "Iteration 4, loss = 6.29844335\n",
      "Iteration 5, loss = 10.37424082\n",
      "Iteration 6, loss = 5.68161515\n",
      "Iteration 7, loss = 8.85088928\n",
      "Iteration 8, loss = 6.71120623\n",
      "Iteration 9, loss = 5.85937852\n",
      "Iteration 10, loss = 7.58362144\n",
      "Iteration 11, loss = 5.74863546\n",
      "Iteration 12, loss = 5.83543903\n",
      "Iteration 13, loss = 6.79158685\n",
      "Iteration 14, loss = 5.35873528\n",
      "Iteration 15, loss = 5.64911159\n",
      "Iteration 16, loss = 6.21231545\n",
      "Iteration 17, loss = 5.20640355\n",
      "Iteration 18, loss = 5.38094762\n",
      "Iteration 19, loss = 5.82307933\n",
      "Iteration 20, loss = 5.06674082\n",
      "Iteration 21, loss = 5.19690536\n",
      "Iteration 22, loss = 5.52087117\n",
      "Iteration 23, loss = 4.97353376\n",
      "Iteration 24, loss = 5.04118388\n",
      "Iteration 25, loss = 5.29031963\n",
      "Iteration 26, loss = 4.88921507\n",
      "Iteration 27, loss = 4.99689521\n",
      "Iteration 28, loss = 5.00078524\n",
      "Iteration 29, loss = 4.77064233\n",
      "Iteration 30, loss = 4.80403559\n",
      "Iteration 31, loss = 4.86699661\n",
      "Iteration 32, loss = 4.70915842\n",
      "Iteration 33, loss = 4.70919762\n",
      "Iteration 34, loss = 4.76978394\n",
      "Iteration 35, loss = 4.65830756\n",
      "Iteration 36, loss = 4.63082427\n",
      "Iteration 37, loss = 4.68070113\n",
      "Iteration 38, loss = 4.59642635\n",
      "Iteration 39, loss = 4.56639000\n",
      "Iteration 40, loss = 4.60413880\n",
      "Iteration 41, loss = 4.53608700\n",
      "Iteration 42, loss = 4.51580625\n",
      "Iteration 43, loss = 4.53697154\n",
      "Iteration 44, loss = 4.47778565\n",
      "Iteration 45, loss = 4.47461302\n",
      "Iteration 46, loss = 4.46929308\n",
      "Iteration 47, loss = 4.42528073\n",
      "Iteration 48, loss = 4.43803811\n",
      "Iteration 49, loss = 4.40529144\n",
      "Iteration 50, loss = 4.39270642\n",
      "Iteration 51, loss = 4.38597542\n",
      "Iteration 52, loss = 4.35676601\n",
      "Iteration 53, loss = 4.35559034\n",
      "Iteration 54, loss = 4.33461006\n",
      "Iteration 55, loss = 4.31843406\n",
      "Iteration 56, loss = 4.30253956\n",
      "Iteration 57, loss = 4.29339802\n",
      "Iteration 58, loss = 4.27509889\n",
      "Iteration 59, loss = 4.25253346\n",
      "Iteration 60, loss = 4.25277100\n",
      "Iteration 61, loss = 4.22827898\n",
      "Iteration 62, loss = 4.19777623\n",
      "Iteration 63, loss = 4.19501670\n",
      "Iteration 64, loss = 4.20920529\n",
      "Iteration 65, loss = 4.18076348\n",
      "Iteration 66, loss = 4.13951504\n",
      "Iteration 67, loss = 4.11969484\n",
      "Iteration 68, loss = 4.12818853\n",
      "Iteration 69, loss = 4.13542318\n",
      "Iteration 70, loss = 4.09511563\n",
      "Iteration 71, loss = 4.02498823\n",
      "Iteration 72, loss = 3.99674801\n",
      "Iteration 73, loss = 4.06848185\n",
      "Iteration 74, loss = 4.48428789\n",
      "Iteration 75, loss = 4.56263837\n",
      "Iteration 76, loss = 3.98681307\n",
      "Iteration 77, loss = 3.97810265\n",
      "Iteration 78, loss = 3.99669982\n",
      "Iteration 79, loss = 3.89873041\n",
      "Iteration 80, loss = 3.93836530\n",
      "Iteration 81, loss = 3.88927082\n",
      "Iteration 82, loss = 3.90165641\n",
      "Iteration 83, loss = 3.97682565\n",
      "Iteration 84, loss = 3.85197646\n",
      "Iteration 85, loss = 3.85813935\n",
      "Iteration 86, loss = 3.89097759\n",
      "Iteration 87, loss = 3.80147424\n",
      "Iteration 88, loss = 3.78000854\n",
      "Iteration 89, loss = 3.83252089\n",
      "Iteration 90, loss = 4.04794548\n",
      "Iteration 91, loss = 3.87931878\n",
      "Iteration 92, loss = 3.72518204\n",
      "Iteration 93, loss = 3.88076430\n",
      "Iteration 94, loss = 3.72086359\n",
      "Iteration 95, loss = 3.72859924\n",
      "Iteration 96, loss = 3.77421148\n",
      "Iteration 97, loss = 3.62834997\n",
      "Iteration 98, loss = 3.70650714\n",
      "Iteration 99, loss = 3.67065841\n",
      "Iteration 100, loss = 3.56799840\n",
      "Iteration 101, loss = 3.64015453\n",
      "Iteration 102, loss = 3.60554097\n",
      "Iteration 103, loss = 3.49741108\n",
      "Iteration 104, loss = 3.52550941\n",
      "Iteration 105, loss = 3.56432539\n",
      "Iteration 106, loss = 3.48913736\n",
      "Iteration 107, loss = 3.39514562\n",
      "Iteration 108, loss = 3.36546359\n",
      "Iteration 109, loss = 3.40172625\n",
      "Iteration 110, loss = 3.52698150\n",
      "Iteration 111, loss = 3.78286649\n",
      "Iteration 112, loss = 4.04568394\n",
      "Iteration 113, loss = 3.60305103\n",
      "Iteration 114, loss = 3.21069371\n",
      "Iteration 115, loss = 3.43698739\n",
      "Iteration 116, loss = 3.52648692\n",
      "Iteration 117, loss = 3.23644001\n",
      "Iteration 118, loss = 3.14711702\n",
      "Iteration 119, loss = 3.38409379\n",
      "Iteration 120, loss = 3.52178772\n",
      "Iteration 121, loss = 3.26234661\n",
      "Iteration 122, loss = 3.03098536\n",
      "Iteration 123, loss = 3.04556672\n",
      "Iteration 124, loss = 3.23362845\n",
      "Iteration 125, loss = 3.46211237\n",
      "Iteration 126, loss = 3.44996826\n",
      "Iteration 127, loss = 3.26181568\n",
      "Iteration 128, loss = 2.95181422\n",
      "Iteration 129, loss = 2.86271225\n",
      "Iteration 130, loss = 3.00616662\n",
      "Iteration 131, loss = 3.21572588\n",
      "Iteration 132, loss = 3.43438848\n",
      "Iteration 133, loss = 3.32231842\n",
      "Iteration 134, loss = 3.07181291\n",
      "Iteration 135, loss = 2.77852901\n",
      "Iteration 136, loss = 2.80084545\n",
      "Iteration 137, loss = 3.03791272\n",
      "Iteration 138, loss = 3.15224734\n",
      "Iteration 139, loss = 3.19341525\n",
      "Iteration 140, loss = 2.92026374\n",
      "Iteration 141, loss = 2.71051732\n",
      "Iteration 142, loss = 2.62985084\n",
      "Iteration 143, loss = 2.70822774\n",
      "Iteration 144, loss = 2.94782729\n",
      "Iteration 145, loss = 3.33756084\n",
      "Iteration 146, loss = 4.17739470\n",
      "Iteration 147, loss = 3.84711440\n",
      "Iteration 148, loss = 2.98298923\n",
      "Iteration 149, loss = 2.76322209\n",
      "Iteration 150, loss = 3.31328568\n",
      "Iteration 151, loss = 2.82913564\n",
      "Iteration 152, loss = 2.84199681\n",
      "Iteration 153, loss = 3.10496698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 11.90161129\n",
      "Iteration 2, loss = 110.34250178\n",
      "Iteration 3, loss = 24.98667011\n",
      "Iteration 4, loss = 0.32753045\n",
      "Iteration 5, loss = 3.26382088\n",
      "Iteration 6, loss = 3.48169424\n",
      "Iteration 7, loss = 2.19124875\n",
      "Iteration 8, loss = 0.81661937\n",
      "Iteration 9, loss = 0.29859149\n",
      "Iteration 10, loss = 0.72264562\n",
      "Iteration 11, loss = 1.04731490\n",
      "Iteration 12, loss = 0.86327996\n",
      "Iteration 13, loss = 0.41693655\n",
      "Iteration 14, loss = 0.30190129\n",
      "Iteration 15, loss = 0.52376021\n",
      "Iteration 16, loss = 0.66969698\n",
      "Iteration 17, loss = 0.58804524\n",
      "Iteration 18, loss = 0.40568241\n",
      "Iteration 19, loss = 0.29177026\n",
      "Iteration 20, loss = 0.33130033\n",
      "Iteration 21, loss = 0.40067376\n",
      "Iteration 22, loss = 0.38878110\n",
      "Iteration 23, loss = 0.34236439\n",
      "Iteration 24, loss = 0.30170130\n",
      "Iteration 25, loss = 0.28225311\n",
      "Iteration 26, loss = 0.28845877\n",
      "Iteration 27, loss = 0.30872911\n",
      "Iteration 28, loss = 0.32052656\n",
      "Iteration 29, loss = 0.31306512\n",
      "Iteration 30, loss = 0.29136945\n",
      "Iteration 31, loss = 0.28143011\n",
      "Iteration 32, loss = 0.30039864\n",
      "Iteration 33, loss = 0.29812450\n",
      "Iteration 34, loss = 0.28407693\n",
      "Iteration 35, loss = 0.28071260\n",
      "Iteration 36, loss = 0.28694254\n",
      "Iteration 37, loss = 0.29119596\n",
      "Iteration 38, loss = 0.28877036\n",
      "Iteration 39, loss = 0.28271858\n",
      "Iteration 40, loss = 0.27950632\n",
      "Iteration 41, loss = 0.28250894\n",
      "Iteration 42, loss = 0.28489426\n",
      "Iteration 43, loss = 0.28329147\n",
      "Iteration 44, loss = 0.28019030\n",
      "Iteration 45, loss = 0.27895287\n",
      "Iteration 46, loss = 0.27901288\n",
      "Iteration 47, loss = 0.27869901\n",
      "Iteration 48, loss = 0.27855182\n",
      "Iteration 49, loss = 0.27789476\n",
      "Iteration 50, loss = 0.27758815\n",
      "Iteration 51, loss = 0.27729751\n",
      "Iteration 52, loss = 0.27627093\n",
      "Iteration 53, loss = 0.27638953\n",
      "Iteration 54, loss = 0.28226652\n",
      "Iteration 55, loss = 0.27491022\n",
      "Iteration 56, loss = 0.27836386\n",
      "Iteration 57, loss = 0.27306553\n",
      "Iteration 58, loss = 0.27672688\n",
      "Iteration 59, loss = 0.27193106\n",
      "Iteration 60, loss = 0.27807406\n",
      "Iteration 61, loss = 0.27073834\n",
      "Iteration 62, loss = 0.27534403\n",
      "Iteration 63, loss = 0.27062883\n",
      "Iteration 64, loss = 0.26946445\n",
      "Iteration 65, loss = 0.27606625\n",
      "Iteration 66, loss = 0.27676219\n",
      "Iteration 67, loss = 0.26758848\n",
      "Iteration 68, loss = 0.27934088\n",
      "Iteration 69, loss = 0.26807722\n",
      "Iteration 70, loss = 0.26990165\n",
      "Iteration 71, loss = 0.27064237\n",
      "Iteration 72, loss = 0.26618212\n",
      "Iteration 73, loss = 0.26527048\n",
      "Iteration 74, loss = 0.26603367\n",
      "Iteration 75, loss = 0.27015495\n",
      "Iteration 76, loss = 0.26733769\n",
      "Iteration 77, loss = 0.26396738\n",
      "Iteration 78, loss = 0.26898015\n",
      "Iteration 79, loss = 0.27942873\n",
      "Iteration 80, loss = 0.26355802\n",
      "Iteration 81, loss = 0.27242342\n",
      "Iteration 82, loss = 0.26333877\n",
      "Iteration 83, loss = 0.27008007\n",
      "Iteration 84, loss = 0.26291888\n",
      "Iteration 85, loss = 0.26655115\n",
      "Iteration 86, loss = 0.26384009\n",
      "Iteration 87, loss = 0.26374609\n",
      "Iteration 88, loss = 0.26420439\n",
      "Iteration 89, loss = 0.26217865\n",
      "Iteration 90, loss = 0.26290993\n",
      "Iteration 91, loss = 0.26136985\n",
      "Iteration 92, loss = 0.26181680\n",
      "Iteration 93, loss = 0.26119192\n",
      "Iteration 94, loss = 0.26067751\n",
      "Iteration 95, loss = 0.26076001\n",
      "Iteration 96, loss = 0.25982858\n",
      "Iteration 97, loss = 0.26014270\n",
      "Iteration 98, loss = 0.25914075\n",
      "Iteration 99, loss = 0.25946656\n",
      "Iteration 100, loss = 0.25846240\n",
      "Iteration 101, loss = 0.25871066\n",
      "Iteration 102, loss = 0.25787129\n",
      "Iteration 103, loss = 0.25786718\n",
      "Iteration 104, loss = 0.25756740\n",
      "Iteration 105, loss = 0.25679530\n",
      "Iteration 106, loss = 0.25695640\n",
      "Iteration 107, loss = 0.25631137\n",
      "Iteration 108, loss = 0.25572244\n",
      "Iteration 109, loss = 0.25574001\n",
      "Iteration 110, loss = 0.25564913\n",
      "Iteration 111, loss = 0.25493864\n",
      "Iteration 112, loss = 0.25427639\n",
      "Iteration 113, loss = 0.25387007\n",
      "Iteration 114, loss = 0.25376079\n",
      "Iteration 115, loss = 0.25425930\n",
      "Iteration 116, loss = 0.25484607\n",
      "Iteration 117, loss = 0.25637435\n",
      "Iteration 118, loss = 0.25391696\n",
      "Iteration 119, loss = 0.25152018\n",
      "Iteration 120, loss = 0.25169460\n",
      "Iteration 121, loss = 0.25355289\n",
      "Iteration 122, loss = 0.25560614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 123, loss = 0.25355896\n",
      "Iteration 124, loss = 0.25017309\n",
      "Iteration 125, loss = 0.24932516\n",
      "Iteration 126, loss = 0.25113342\n",
      "Iteration 127, loss = 0.25199818\n",
      "Iteration 128, loss = 0.24988296\n",
      "Iteration 129, loss = 0.24770040\n",
      "Iteration 130, loss = 0.24686307\n",
      "Iteration 131, loss = 0.24769704\n",
      "Iteration 132, loss = 0.24970435\n",
      "Iteration 133, loss = 0.25024931\n",
      "Iteration 134, loss = 0.25110292\n",
      "Iteration 135, loss = 0.24754103\n",
      "Iteration 136, loss = 0.24540497\n",
      "Iteration 137, loss = 0.24429052\n",
      "Iteration 138, loss = 0.24444638\n",
      "Iteration 139, loss = 0.24595634\n",
      "Iteration 140, loss = 0.24866737\n",
      "Iteration 141, loss = 0.25740238\n",
      "Iteration 142, loss = 0.25375752\n",
      "Iteration 143, loss = 0.24862723\n",
      "Iteration 144, loss = 0.24283447\n",
      "Iteration 145, loss = 0.24852844\n",
      "Iteration 146, loss = 0.25382283\n",
      "Iteration 147, loss = 0.24285375\n",
      "Iteration 148, loss = 0.24796800\n",
      "Iteration 149, loss = 0.25526744\n",
      "Iteration 150, loss = 0.24185450\n",
      "Iteration 151, loss = 0.25172232\n",
      "Iteration 152, loss = 0.25571722\n",
      "Iteration 153, loss = 0.24440558\n",
      "Iteration 154, loss = 0.26228678\n",
      "Iteration 155, loss = 0.25259449\n",
      "Iteration 156, loss = 0.25574082\n",
      "Iteration 157, loss = 0.24701154\n",
      "Iteration 158, loss = 0.25345621\n",
      "Iteration 159, loss = 0.24851726\n",
      "Iteration 160, loss = 0.25315005\n",
      "Iteration 161, loss = 0.24404445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from scipy.stats import logistic\n",
    "\n",
    "MC_N = 1\n",
    "MC_θ = np.zeros((MC_N,4))\n",
    "MC_y = np.zeros((MC_N,4))\n",
    "MC_t = np.zeros((MC_N,4))\n",
    "\n",
    "for j in range(MC_N):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    df = pd.read_csv('/Users/pranjal/Desktop/Causal-Inference/data/wage.csv')\n",
    "    cat = df.select_dtypes('object').columns\n",
    "    df = pd.get_dummies(df, columns = cat, drop_first = True)\n",
    "    outcome = 'lwage'\n",
    "    treatment = 'educ'\n",
    "    #rest = list(df.drop([outcome, treatment], axis = 1).columns)\n",
    "    rest = ['exper','age', 'kidslt6', 'kidsge6']\n",
    "    df = df[[outcome] + [treatment] + rest]\n",
    "    df = df.dropna()\n",
    "    y = df[outcome]\n",
    "    t = df[treatment]\n",
    "    x = df[rest].astype('float')\n",
    "    print(x.shape, t.shape, y.shape)\n",
    "    \n",
    "    # OLS - Full Estimation\n",
    "    model_OLS = sm.OLS(y, np.c_[t,x])\n",
    "    res = model_OLS.fit()\n",
    "    θ_OLS = res.params[0]\n",
    "    # OLS First Stage: Y\n",
    "    model_OLS = sm.OLS(y, np.c_[x])\n",
    "    res_y = model_OLS.fit()\n",
    "    θ_OLS_y = res_y.params[0]    \n",
    "    # Logistic First Stage\n",
    "    clf = LinearRegression().fit(x, t)\n",
    "    \n",
    "    # DML Lasso\n",
    "    model_Lasso = LinearDML(discrete_treatment=False, random_state=1, cv=1)\n",
    "    model_Lasso.fit(y.ravel(), t.ravel(), X=None,W=x)\n",
    "    θ_DMLL = model_Lasso.intercept_\n",
    "\n",
    "    # DML RF\n",
    "    model_XGB = LinearDML(discrete_treatment=False, cv=1,\n",
    "                          model_y = CatBoostRegressor(learning_rate = 1, max_depth = 8,verbose = False), \n",
    "                          model_t = CatBoostRegressor(learning_rate = 1, max_depth = 8,verbose = False))\n",
    "    model_XGB.fit(y.ravel(), t.ravel(), X=None,W=x)\n",
    "    θ_DMLRF = model_XGB.intercept_\n",
    "    \n",
    "    # DML NN - First Stage\n",
    "    model_NN = LinearDML(discrete_treatment=False, cv =1,\n",
    "                         model_y = MLPRegressor(random_state=1,\n",
    "                                                 hidden_layer_sizes=(500,100,50), \n",
    "                                                 batch_size = x.shape[0],\n",
    "                                                 momentum = 0.95, \n",
    "                                                 max_iter=50000, \n",
    "                                                 learning_rate_init=0.01, \n",
    "                                                 verbose=True), \n",
    "                         model_t = MLPRegressor(random_state=1,\n",
    "                                                 hidden_layer_sizes=(500,100,50), \n",
    "                                                 batch_size = x.shape[0],\n",
    "                                                 momentum = 0.95, \n",
    "                                                 max_iter=50000, \n",
    "                                                 learning_rate_init=0.01, \n",
    "                                                 verbose=True))\n",
    "    model_NN.fit(y.ravel(), t.ravel(), X=None,W=x)\n",
    "    θ_DMLRF = model_NN.intercept_\n",
    "\n",
    "\n",
    "    MC_θ[j] = [θ_OLS, model_Lasso.intercept_, model_XGB.intercept_, model_NN.intercept_]\n",
    "    MC_y[j] = [res_y.rsquared, np.mean(model_Lasso.nuisance_scores_y), np.mean(model_XGB.nuisance_scores_y),np.mean(model_NN.nuisance_scores_y)]\n",
    "    MC_t[j] = [clf.score(x,t), np.mean(model_Lasso.nuisance_scores_t), np.mean(model_XGB.nuisance_scores_t),np.mean(model_NN.nuisance_scores_t)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31aebd79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-------+-----------+--------+\n",
      "|       Var        |  OLS  | DML-L | DML-Boost | DML-NN |\n",
      "+------------------+-------+-------+-----------+--------+\n",
      "|      θ_hat       | 0.103 | 0.110 |   0.124   | 0.101  |\n",
      "| First Stage Y R2 | 0.723 | 0.027 |   0.924   | 0.032  |\n",
      "| First Stage D R2 | 0.031 | 0.031 |   0.866   | -0.024 |\n",
      "+------------------+-------+-------+-----------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Var', 'OLS','DML-L','DML-Boost','DML-NN']\n",
    "a = ['θ_hat']+ np.mean(MC_θ, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "a = ['First Stage Y R2']+ np.mean(MC_y, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "a = ['First Stage D R2']+ np.mean(MC_t, axis = 0).tolist()\n",
    "table.add_row(a)\n",
    "table.float_format = '0.3'\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b847c801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>lwage</td>      <th>  R-squared (uncentered):</th>      <td>   0.771</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.768</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   285.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 09 Dec 2022</td> <th>  Prob (F-statistic):</th>          <td>5.93e-133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:56:25</td>     <th>  Log-Likelihood:    </th>          <td> -433.42</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   428</td>      <th>  AIC:               </th>          <td>   876.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   423</td>      <th>  BIC:               </th>          <td>   897.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.1027</td> <td>    0.011</td> <td>    9.427</td> <td> 0.000</td> <td>    0.081</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.0155</td> <td>    0.005</td> <td>    3.274</td> <td> 0.001</td> <td>    0.006</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>   -0.0062</td> <td>    0.004</td> <td>   -1.745</td> <td> 0.082</td> <td>   -0.013</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   -0.0881</td> <td>    0.087</td> <td>   -1.013</td> <td> 0.312</td> <td>   -0.259</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>   -0.0267</td> <td>    0.025</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.076</td> <td>    0.022</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>77.868</td> <th>  Durbin-Watson:     </th> <td>   1.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 298.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.757</td> <th>  Prob(JB):          </th> <td>1.65e-65</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.800</td> <th>  Cond. No.          </th> <td>    125.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                  lwage   R-squared (uncentered):                   0.771\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.768\n",
       "Method:                 Least Squares   F-statistic:                              285.0\n",
       "Date:                Fri, 09 Dec 2022   Prob (F-statistic):                   5.93e-133\n",
       "Time:                        17:56:25   Log-Likelihood:                         -433.42\n",
       "No. Observations:                 428   AIC:                                      876.8\n",
       "Df Residuals:                     423   BIC:                                      897.1\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.1027      0.011      9.427      0.000       0.081       0.124\n",
       "x2             0.0155      0.005      3.274      0.001       0.006       0.025\n",
       "x3            -0.0062      0.004     -1.745      0.082      -0.013       0.001\n",
       "x4            -0.0881      0.087     -1.013      0.312      -0.259       0.083\n",
       "x5            -0.0267      0.025     -1.070      0.285      -0.076       0.022\n",
       "==============================================================================\n",
       "Omnibus:                       77.868   Durbin-Watson:                   1.987\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              298.330\n",
       "Skew:                          -0.757   Prob(JB):                     1.65e-65\n",
       "Kurtosis:                       6.800   Cond. No.                         125.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "686afd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>CATE Intercept Results</caption>\n",
       "<tr>\n",
       "         <td></td>        <th>point_estimate</th> <th>stderr</th> <th>zstat</th> <th>pvalue</th> <th>ci_lower</th> <th>ci_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cate_intercept</th>      <td>0.124</td>      <td>0.023</td> <td>5.492</td>   <td>0.0</td>    <td>0.08</td>     <td>0.168</td> \n",
       "</tr>\n",
       "</table><br/><br/><sub>A linear parametric conditional average treatment effect (CATE) model was fitted:<br/>$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$<br/>where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:<br/>$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$<br/>Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>"
      ],
      "text/plain": [
       "<class 'econml.utilities.Summary'>\n",
       "\"\"\"\n",
       "                       CATE Intercept Results                      \n",
       "===================================================================\n",
       "               point_estimate stderr zstat pvalue ci_lower ci_upper\n",
       "-------------------------------------------------------------------\n",
       "cate_intercept          0.124  0.023 5.492    0.0     0.08    0.168\n",
       "-------------------------------------------------------------------\n",
       "\n",
       "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
       "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
       "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
       "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
       "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_XGB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7ce2371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_cached_values',\n",
       " '_check_fitted_dims',\n",
       " '_check_fitted_dims_w_z',\n",
       " '_check_input_dims',\n",
       " '_d_t',\n",
       " '_d_t_in',\n",
       " '_d_w',\n",
       " '_d_x',\n",
       " '_d_y',\n",
       " '_d_z',\n",
       " '_defer_to_inference',\n",
       " '_expand_treatments',\n",
       " '_fit_final',\n",
       " '_fit_nuisances',\n",
       " '_gen_featurizer',\n",
       " '_gen_model_final',\n",
       " '_gen_model_t',\n",
       " '_gen_model_y',\n",
       " '_gen_ortho_learner_model_final',\n",
       " '_gen_ortho_learner_model_nuisance',\n",
       " '_gen_rlearner_model_final',\n",
       " '_get_inference',\n",
       " '_get_inference_options',\n",
       " '_illegal_refit_inference_methods',\n",
       " '_inference',\n",
       " '_input_names',\n",
       " '_models_nuisance',\n",
       " '_original_treatment_featurizer',\n",
       " '_ortho_learner_model_final',\n",
       " '_ortho_learner_model_nuisance',\n",
       " '_postfit',\n",
       " '_prefit',\n",
       " '_random_state',\n",
       " '_set_input_names',\n",
       " '_set_transformed_treatment_names',\n",
       " '_strata',\n",
       " '_subinds_check_none',\n",
       " '_use_inference_method',\n",
       " '_wrap_fit',\n",
       " 'ate',\n",
       " 'ate_inference',\n",
       " 'ate_interval',\n",
       " 'bias_part_of_coef',\n",
       " 'cate_feature_names',\n",
       " 'cate_output_names',\n",
       " 'cate_treatment_names',\n",
       " 'categories',\n",
       " 'coef_',\n",
       " 'coef__inference',\n",
       " 'coef__interval',\n",
       " 'const_marginal_ate',\n",
       " 'const_marginal_ate_inference',\n",
       " 'const_marginal_ate_interval',\n",
       " 'const_marginal_effect',\n",
       " 'const_marginal_effect_inference',\n",
       " 'const_marginal_effect_interval',\n",
       " 'cv',\n",
       " 'discrete_instrument',\n",
       " 'discrete_treatment',\n",
       " 'dowhy',\n",
       " 'effect',\n",
       " 'effect_inference',\n",
       " 'effect_interval',\n",
       " 'featurizer',\n",
       " 'featurizer_',\n",
       " 'fit',\n",
       " 'fit_cate_intercept',\n",
       " 'fit_cate_intercept_',\n",
       " 'intercept_',\n",
       " 'intercept__inference',\n",
       " 'intercept__interval',\n",
       " 'linear_first_stages',\n",
       " 'marginal_ate',\n",
       " 'marginal_ate_inference',\n",
       " 'marginal_ate_interval',\n",
       " 'marginal_effect',\n",
       " 'marginal_effect_inference',\n",
       " 'marginal_effect_interval',\n",
       " 'mc_agg',\n",
       " 'mc_iters',\n",
       " 'model_cate',\n",
       " 'model_final',\n",
       " 'model_final_',\n",
       " 'model_t',\n",
       " 'model_y',\n",
       " 'models_nuisance_',\n",
       " 'models_t',\n",
       " 'models_y',\n",
       " 'nuisance_scores_',\n",
       " 'nuisance_scores_t',\n",
       " 'nuisance_scores_y',\n",
       " 'original_featurizer',\n",
       " 'ortho_learner_model_final_',\n",
       " 'random_state',\n",
       " 'refit_final',\n",
       " 'residuals_',\n",
       " 'rlearner_model_final_',\n",
       " 'score',\n",
       " 'score_',\n",
       " 'shap_values',\n",
       " 'summary',\n",
       " 'transformer',\n",
       " 'treatment_featurizer',\n",
       " 'z_transformer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8a9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
